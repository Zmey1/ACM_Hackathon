{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P7FSTr1yNwJR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W5F6XwvIN0kG"
      },
      "outputs": [],
      "source": [
        "soil_types = ['Red Soil', 'Black Clayey Soil', 'Brown Soil']\n",
        "\n",
        "# Defining ranges for each property (example ranges, these can be adjusted with actual data)\n",
        "soil_properties = {\n",
        "    'Water Holding Capacity (mm/m)': {\n",
        "        'Red Soil': (200, 300),\n",
        "        'Black Clayey Soil': (350, 450),\n",
        "        'Brown Soil': (250, 350)\n",
        "    },\n",
        "    'Field Capacity (%)': {\n",
        "        'Red Soil': (20, 25),\n",
        "        'Black Clayey Soil': (30, 40),\n",
        "        'Brown Soil': (25, 35)\n",
        "    },\n",
        "    'Permanent Wilting Point (%)': {\n",
        "        'Red Soil': (10, 15),\n",
        "        'Black Clayey Soil': (15, 20),\n",
        "        'Brown Soil': (12, 17)\n",
        "    },\n",
        "    'Infiltration Rate (mm/hour)': {\n",
        "        'Red Soil': (1.5, 2.5),\n",
        "        'Black Clayey Soil': (0.5, 1.0),\n",
        "        'Brown Soil': (1.0, 2.0)\n",
        "    },\n",
        "    'Soil Texture Class (Sandy, Silty, Clay %)': {\n",
        "        'Red Soil': (70, 10, 20),  # e.g., 70% sandy, 10% silty, 20% clay\n",
        "        'Black Clayey Soil': (40, 20, 40),\n",
        "        'Brown Soil': (50, 25, 25)\n",
        "    },\n",
        "    'Bulk Density (g/cm³)': {\n",
        "        'Red Soil': (1.2, 1.4),\n",
        "        'Black Clayey Soil': (1.3, 1.5),\n",
        "        'Brown Soil': (1.2, 1.4)\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "7G-6VOFqN3U5",
        "outputId": "c201c35e-6073-497a-d5e5-3a780a82015b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"synthetic_data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Soil Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Black Clayey Soil\",\n          \"Brown Soil\",\n          \"Red Soil\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Water Holding Capacity (mm/m)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 67.14614265758091,\n        \"min\": 201.50963858165068,\n        \"max\": 449.4451470888304,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          217.1264738338172,\n          408.7878958723798,\n          279.12693954951607\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Field Capacity (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.752259186647536,\n        \"min\": 20.003128133504685,\n        \"max\": 39.97660750230126,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          20.65735922005137,\n          35.28532338567417,\n          23.927544514951485\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Permanent Wilting Point (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.491500005610133,\n        \"min\": 10.000750015956488,\n        \"max\": 19.999611370532577,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          10.438395303300238,\n          15.070004047150714,\n          10.88496473638985\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Infiltration Rate (mm/hour)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5800203623561597,\n        \"min\": 0.5007105490212659,\n        \"max\": 2.4983121121887755,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          1.673442437860192,\n          0.6086162477489873,\n          1.9709282140031075\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sandy (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 40,\n        \"max\": 70,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          40,\n          50,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Silty (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 10,\n        \"max\": 25,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          20,\n          25,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Clay (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 20,\n        \"max\": 40,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          40,\n          25,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bulk Density (g/cm\\u00b3)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07402581776216559,\n        \"min\": 1.2002813593403217,\n        \"max\": 1.4997411713354898,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          1.2225834028775993,\n          1.3155539957218156,\n          1.2290529110466732\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "synthetic_data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bce3e73f-790c-4eb2-bc3c-7001a1e97bed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Soil Type</th>\n",
              "      <th>Water Holding Capacity (mm/m)</th>\n",
              "      <th>Field Capacity (%)</th>\n",
              "      <th>Permanent Wilting Point (%)</th>\n",
              "      <th>Infiltration Rate (mm/hour)</th>\n",
              "      <th>Sandy (%)</th>\n",
              "      <th>Silty (%)</th>\n",
              "      <th>Clay (%)</th>\n",
              "      <th>Bulk Density (g/cm³)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Black Clayey Soil</td>\n",
              "      <td>439.244233</td>\n",
              "      <td>30.729889</td>\n",
              "      <td>18.546972</td>\n",
              "      <td>0.816463</td>\n",
              "      <td>40</td>\n",
              "      <td>20</td>\n",
              "      <td>40</td>\n",
              "      <td>1.373647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Black Clayey Soil</td>\n",
              "      <td>378.790632</td>\n",
              "      <td>34.984335</td>\n",
              "      <td>16.513733</td>\n",
              "      <td>0.882883</td>\n",
              "      <td>40</td>\n",
              "      <td>20</td>\n",
              "      <td>40</td>\n",
              "      <td>1.326452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Brown Soil</td>\n",
              "      <td>288.470601</td>\n",
              "      <td>32.316135</td>\n",
              "      <td>13.189386</td>\n",
              "      <td>1.156767</td>\n",
              "      <td>50</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>1.264088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Red Soil</td>\n",
              "      <td>270.632063</td>\n",
              "      <td>20.314999</td>\n",
              "      <td>13.565113</td>\n",
              "      <td>2.228248</td>\n",
              "      <td>70</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>1.318909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Black Clayey Soil</td>\n",
              "      <td>417.464748</td>\n",
              "      <td>34.023248</td>\n",
              "      <td>16.514994</td>\n",
              "      <td>0.783501</td>\n",
              "      <td>40</td>\n",
              "      <td>20</td>\n",
              "      <td>40</td>\n",
              "      <td>1.377525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bce3e73f-790c-4eb2-bc3c-7001a1e97bed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bce3e73f-790c-4eb2-bc3c-7001a1e97bed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bce3e73f-790c-4eb2-bc3c-7001a1e97bed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fe7f707c-0eb2-41d6-8931-25944db005cb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fe7f707c-0eb2-41d6-8931-25944db005cb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fe7f707c-0eb2-41d6-8931-25944db005cb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           Soil Type  Water Holding Capacity (mm/m)  Field Capacity (%)  \\\n",
              "0  Black Clayey Soil                     439.244233           30.729889   \n",
              "1  Black Clayey Soil                     378.790632           34.984335   \n",
              "2         Brown Soil                     288.470601           32.316135   \n",
              "3           Red Soil                     270.632063           20.314999   \n",
              "4  Black Clayey Soil                     417.464748           34.023248   \n",
              "\n",
              "   Permanent Wilting Point (%)  Infiltration Rate (mm/hour)  Sandy (%)  \\\n",
              "0                    18.546972                     0.816463         40   \n",
              "1                    16.513733                     0.882883         40   \n",
              "2                    13.189386                     1.156767         50   \n",
              "3                    13.565113                     2.228248         70   \n",
              "4                    16.514994                     0.783501         40   \n",
              "\n",
              "   Silty (%)  Clay (%)  Bulk Density (g/cm³)  \n",
              "0         20        40              1.373647  \n",
              "1         20        40              1.326452  \n",
              "2         25        25              1.264088  \n",
              "3         10        20              1.318909  \n",
              "4         20        40              1.377525  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def generate_synthetic_data(soil_types, soil_properties, num_samples):\n",
        "    data = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        soil_type = np.random.choice(soil_types)\n",
        "\n",
        "        # Generate data for each property based on the soil type\n",
        "        water_holding_capacity = np.random.uniform(*soil_properties['Water Holding Capacity (mm/m)'][soil_type])\n",
        "        field_capacity = np.random.uniform(*soil_properties['Field Capacity (%)'][soil_type])\n",
        "        wilting_point = np.random.uniform(*soil_properties['Permanent Wilting Point (%)'][soil_type])\n",
        "        infiltration_rate = np.random.uniform(*soil_properties['Infiltration Rate (mm/hour)'][soil_type])\n",
        "\n",
        "        # Soil texture (sandy, silty, clay %)\n",
        "        sandy, silty, clay = soil_properties['Soil Texture Class (Sandy, Silty, Clay %)'][soil_type]\n",
        "\n",
        "        # Generate bulk density\n",
        "        bulk_density = np.random.uniform(*soil_properties['Bulk Density (g/cm³)'][soil_type])\n",
        "\n",
        "        # Append the data as a row\n",
        "        data.append([\n",
        "            soil_type,\n",
        "            water_holding_capacity,\n",
        "            field_capacity,\n",
        "            wilting_point,\n",
        "            infiltration_rate,\n",
        "            sandy, silty, clay,\n",
        "            bulk_density\n",
        "        ])\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data, columns=[\n",
        "        'Soil Type',\n",
        "        'Water Holding Capacity (mm/m)',\n",
        "        'Field Capacity (%)',\n",
        "        'Permanent Wilting Point (%)',\n",
        "        'Infiltration Rate (mm/hour)',\n",
        "        'Sandy (%)', 'Silty (%)', 'Clay (%)',\n",
        "        'Bulk Density (g/cm³)'\n",
        "    ])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Generate synthetic dataset\n",
        "num_samples = 1000  # Define how many samples you want to generate\n",
        "synthetic_data = generate_synthetic_data(soil_types, soil_properties, num_samples)\n",
        "synthetic_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c6IaQgb4N8AW"
      },
      "outputs": [],
      "source": [
        "# GAN model setup\n",
        "def build_generator(latent_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, input_dim=latent_dim))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dense(512))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dense(8, activation='tanh'))  # 8 values output for our soil data\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_discriminator(input_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(512, input_shape=input_shape))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = models.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE3WWgQKN_FB",
        "outputId": "3cbff537-44d7-4795-ca7b-a2e75a9aa8bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "latent_dim = 100  # Size of the random input vector to the generator\n",
        "\n",
        "# Build the generator and discriminator\n",
        "generator = build_generator(latent_dim)\n",
        "discriminator = build_discriminator((8,))  # 8 features as input\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build and compile the GAN\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(optimizer='adam', loss='binary_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bAhWaVZ3OaRn"
      },
      "outputs": [],
      "source": [
        "# Compile the discriminator separately\n",
        "discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Compile the GAN by freezing the discriminator (no training for discriminator during GAN training)\n",
        "gan.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "            loss='binary_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FkQnK8r8Od4L"
      },
      "outputs": [],
      "source": [
        "# Optimized training loop with smaller epochs and learning rate adjustment\n",
        "def train_gan(epochs, batch_size, latent_dim, X_train, generator, discriminator, gan):\n",
        "    batch_count = X_train.shape[0] // batch_size\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(batch_count):\n",
        "            # Train Discriminator\n",
        "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "            real_samples = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
        "            fake_samples = generator.predict(noise)\n",
        "\n",
        "            # Train discriminator on real and fake samples\n",
        "            d_loss_real = discriminator.train_on_batch(real_samples, np.ones((half_batch, 1)))\n",
        "            d_loss_fake = discriminator.train_on_batch(fake_samples, np.zeros((half_batch, 1)))\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Train Generator (we want to trick the discriminator into classifying fake samples as real)\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))  # Label for generator is 1 (real)\n",
        "\n",
        "        # Learning rate scheduler (reducing the learning rate over time)\n",
        "        if epoch % 100 == 0:\n",
        "            # Update the learning rate of the GAN's optimizer\n",
        "            current_lr = gan.optimizer.learning_rate\n",
        "            new_lr = current_lr * 0.95  # Reduce by 5% every 100 epochs\n",
        "            gan.optimizer.learning_rate.assign(new_lr)  # Update the learning rate\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} - Updated learning rate: {new_lr}\")\n",
        "\n",
        "        # Print training progress every few epochs\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'{epoch+1}/{epochs} | D Loss: {d_loss[0]} | G Loss: {g_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "b1GhL_kjOgrI",
        "outputId": "8509da2b-0b52-43d4-cba1-348bc512e1fe"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'normalized_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f93828eb9685>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m   \u001b[0;31m# Reduced epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m  \u001b[0;31m# Smaller batch size for faster training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'normalized_data' is not defined"
          ]
        }
      ],
      "source": [
        "# Train the GAN with optimized settings\n",
        "epochs = 11   # Reduced epochs\n",
        "batch_size = 32  # Smaller batch size for faster training\n",
        "train_gan(epochs, batch_size, latent_dim, normalized_data, generator, discriminator, gan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "EHSJXgpePAIv",
        "outputId": "76043766-745f-4489-a0c8-e67bf2574c67"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_your_soil_data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6ec483ddb2d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Assuming you have a DataFrame with the soil data (e.g., from a CSV file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Replace this with the actual data loading step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path_to_your_soil_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Normalize the numerical features (e.g., soil properties) using MinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_soil_data.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming you have a DataFrame with the soil data (e.g., from a CSV file)\n",
        "# Replace this with the actual data loading step\n",
        "data = pd.read_csv('path_to_your_soil_data.csv')\n",
        "\n",
        "# Normalize the numerical features (e.g., soil properties) using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(data.iloc[:, 1:])  # Exclude non-numeric columns (if any)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAm2fS2hPfE-",
        "outputId": "fb0a89aa-f154-4e93-985e-fbf3f9811a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file saved at: /content/soil_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create sample data (you can replace this with your actual data)\n",
        "data = {\n",
        "    'SoilType': ['Red Soil', 'Black Clayey Soil', 'Brown Soil'],\n",
        "    'WaterHoldingCapacity': [120, 140, 110],\n",
        "    'FieldCapacity': [30, 35, 25],\n",
        "    'PermanentWiltingPoint': [12, 15, 10],\n",
        "    'InfiltrationRate': [5.2, 4.8, 5.5],\n",
        "    'SoilTextureClass': ['Clay', 'Clay', 'Loam'],\n",
        "    'BulkDensity': [1.3, 1.4, 1.35]\n",
        "}\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_file_path = '/content/soil_data.csv'\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"CSV file saved at: {csv_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Whl48UImPqGh",
        "outputId": "10d13106-a420-475c-a962-92cc7631d7f7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'Clay'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a906ef084136>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Normalize the numerical features (e.g., soil properties) using MinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnormalized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exclude non-numeric columns (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Clay'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming you have a DataFrame with the soil data (e.g., from a CSV file)\n",
        "# Replace this with the actual data loading step\n",
        "data = pd.read_csv('/content/soil_data.csv')\n",
        "\n",
        "# Normalize the numerical features (e.g., soil properties) using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(data.iloc[:, 1:])  # Exclude non-numeric columns (if any)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9azMTPpP3Mz",
        "outputId": "8ffabab7-4469-49b3-dd0b-75254b4a82b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            SoilType  WaterHoldingCapacity  FieldCapacity  \\\n",
            "0           Red Soil              0.333333            0.5   \n",
            "1  Black Clayey Soil              1.000000            1.0   \n",
            "2         Brown Soil              0.000000            0.0   \n",
            "\n",
            "   PermanentWiltingPoint  InfiltrationRate SoilTextureClass  BulkDensity  \n",
            "0                    0.4          0.571429             Clay          0.0  \n",
            "1                    1.0          0.000000             Clay          1.0  \n",
            "2                    0.0          1.000000             Loam          0.5  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Sample data (replace this with your actual data if you have a CSV file)\n",
        "data = {\n",
        "    'SoilType': ['Red Soil', 'Black Clayey Soil', 'Brown Soil'],\n",
        "    'WaterHoldingCapacity': [120, 140, 110],\n",
        "    'FieldCapacity': [30, 35, 25],\n",
        "    'PermanentWiltingPoint': [12, 15, 10],\n",
        "    'InfiltrationRate': [5.2, 4.8, 5.5],\n",
        "    'SoilTextureClass': ['Clay', 'Clay', 'Loam'],\n",
        "    'BulkDensity': [1.3, 1.4, 1.35]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Select only numeric columns for normalization\n",
        "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Normalize the numeric columns\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = df.copy()  # Make a copy of the original dataframe\n",
        "\n",
        "# Apply MinMaxScaler to the numeric columns only\n",
        "normalized_data[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
        "\n",
        "# Check the normalized data\n",
        "print(normalized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "ff1uLul_QCG1",
        "outputId": "34d71778-6f81-45da-9d42-16ec9957dec8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11 - Updated learning rate: 0.00018999999156221747\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'd_loss' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8d2c12b83a45>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m  \u001b[0;31m# Smaller batch size for faster training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-8d2c12b83a45>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(epochs, batch_size, latent_dim, X_train, generator, discriminator, gan)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Print training progress every few epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{epoch+1}/{epochs} | D Loss: {d_loss[0]} | G Loss: {g_loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# Instantiate and compile the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'd_loss' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load and normalize the data (already done)\n",
        "# Let's assume 'normalized_data' is available as the dataset\n",
        "\n",
        "# Define the latent dimension (size of the noise vector)\n",
        "latent_dim = 100\n",
        "\n",
        "# Build the generator\n",
        "def build_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(7, activation='linear'))  # 7 features (adjust as necessary)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "    return model\n",
        "\n",
        "# Build the discriminator\n",
        "def build_discriminator(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output probability (real/fake)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GAN (combining generator and discriminator)\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Freeze the discriminator during GAN training\n",
        "\n",
        "    # GAN input: random noise\n",
        "    gan_input = Input(shape=(latent_dim,))\n",
        "    x = generator(gan_input)\n",
        "\n",
        "    # The discriminator evaluates the generated sample\n",
        "    gan_output = discriminator(x)\n",
        "\n",
        "    # Create the GAN model\n",
        "    gan = Model(gan_input, gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "\n",
        "    return gan\n",
        "\n",
        "# Optimized training loop with smaller epochs and learning rate adjustment\n",
        "def train_gan(epochs, batch_size, latent_dim, X_train, generator, discriminator, gan):\n",
        "    batch_count = X_train.shape[0] // batch_size\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(batch_count):\n",
        "            # Train Discriminator\n",
        "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "            real_samples = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
        "            fake_samples = generator.predict(noise)\n",
        "\n",
        "            # Train discriminator on real and fake samples\n",
        "            d_loss_real = discriminator.train_on_batch(real_samples, np.ones((half_batch, 1)))\n",
        "            d_loss_fake = discriminator.train_on_batch(fake_samples, np.zeros((half_batch, 1)))\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Train Generator (we want to trick the discriminator into classifying fake samples as real)\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))  # Label for generator is 1 (real)\n",
        "\n",
        "        # Learning rate scheduler (reducing the learning rate over time)\n",
        "        if epoch % 100 == 0:\n",
        "            current_lr = gan.optimizer.learning_rate\n",
        "            new_lr = current_lr * 0.95  # Reduce by 5% every 100 epochs\n",
        "            gan.optimizer.learning_rate.assign(new_lr)  # Update the learning rate\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} - Updated learning rate: {new_lr}\")\n",
        "\n",
        "        # Print training progress every few epochs\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'{epoch+1}/{epochs} | D Loss: {d_loss[0]} | G Loss: {g_loss}')\n",
        "\n",
        "# Instantiate and compile the models\n",
        "generator = build_generator(latent_dim)\n",
        "discriminator = build_discriminator(normalized_data.shape[1])  # Input shape should match the data\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN with optimized settings\n",
        "epochs = 11   # You can experiment with different values (e.g., 1000 or 2000) based on your needs\n",
        "batch_size = 32  # Smaller batch size for faster training\n",
        "\n",
        "train_gan(epochs, batch_size, latent_dim, normalized_data, generator, discriminator, gan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "zdY-Cl66Qfby",
        "outputId": "666599ef-ea81-4b72-b0f1-4dbcea92a456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5000 - Updated learning rate: 0.00018999999156221747\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'd_loss' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-26033459fe6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m  \u001b[0;31m# Smaller batch size for faster training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-26033459fe6f>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(epochs, batch_size, latent_dim, X_train, generator, discriminator, gan)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Print training progress every few epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{epoch+1}/{epochs} | D Loss: {d_loss} | G Loss: {g_loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# Instantiate and compile the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'd_loss' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load and normalize the data (already done)\n",
        "# Let's assume 'normalized_data' is available as the dataset\n",
        "\n",
        "# Define the latent dimension (size of the noise vector)\n",
        "latent_dim = 100\n",
        "\n",
        "# Build the generator\n",
        "def build_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(7, activation='linear'))  # 7 features (adjust as necessary)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "    return model\n",
        "\n",
        "# Build the discriminator\n",
        "def build_discriminator(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output probability (real/fake)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GAN (combining generator and discriminator)\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Freeze the discriminator during GAN training\n",
        "\n",
        "    # GAN input: random noise\n",
        "    gan_input = Input(shape=(latent_dim,))\n",
        "    x = generator(gan_input)\n",
        "\n",
        "    # The discriminator evaluates the generated sample\n",
        "    gan_output = discriminator(x)\n",
        "\n",
        "    # Create the GAN model\n",
        "    gan = Model(gan_input, gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "\n",
        "    return gan\n",
        "\n",
        "# Optimized training loop with smaller epochs and learning rate adjustment\n",
        "def train_gan(epochs, batch_size, latent_dim, X_train, generator, discriminator, gan):\n",
        "    batch_count = X_train.shape[0] // batch_size\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(batch_count):\n",
        "            # Train Discriminator\n",
        "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "            real_samples = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
        "            fake_samples = generator.predict(noise)\n",
        "\n",
        "            # Train discriminator on real and fake samples\n",
        "            d_loss_real = discriminator.train_on_batch(real_samples, np.ones((half_batch, 1)))\n",
        "            d_loss_fake = discriminator.train_on_batch(fake_samples, np.zeros((half_batch, 1)))\n",
        "\n",
        "            # Correctly calculate d_loss as an average of the real and fake losses\n",
        "            d_loss = 0.5 * np.add(d_loss_real[0], d_loss_fake[0])  # d_loss_real[0] and d_loss_fake[0] are loss values\n",
        "\n",
        "            # Train Generator (we want to trick the discriminator into classifying fake samples as real)\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))  # Label for generator is 1 (real)\n",
        "\n",
        "        # Learning rate scheduler (reducing the learning rate over time)\n",
        "        if epoch % 100 == 0:\n",
        "            current_lr = gan.optimizer.learning_rate\n",
        "            new_lr = current_lr * 0.95  # Reduce by 5% every 100 epochs\n",
        "            gan.optimizer.learning_rate.assign(new_lr)  # Update the learning rate\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} - Updated learning rate: {new_lr}\")\n",
        "\n",
        "        # Print training progress every few epochs\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'{epoch+1}/{epochs} | D Loss: {d_loss} | G Loss: {g_loss}')\n",
        "\n",
        "# Instantiate and compile the models\n",
        "generator = build_generator(latent_dim)\n",
        "discriminator = build_discriminator(normalized_data.shape[1])  # Input shape should match the data\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN with optimized settings\n",
        "epochs = 5000    # You can experiment with different values (e.g., 1000 or 2000) based on your needs\n",
        "batch_size = 32  # Smaller batch size for faster training\n",
        "\n",
        "train_gan(epochs, batch_size, latent_dim, normalized_data, generator, discriminator, gan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_zHpiMUQ0Ar",
        "outputId": "b0cbf4f9-5ef3-4508-9fdf-3c3f2437e6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11 - Updated learning rate: 0.00018999999156221747\n",
            "1/11 | D Loss: 0 | G Loss: 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load and normalize the data (already done)\n",
        "# Let's assume 'normalized_data' is available as the dataset\n",
        "\n",
        "# Define the latent dimension (size of the noise vector)\n",
        "latent_dim = 100\n",
        "\n",
        "# Build the generator\n",
        "def build_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(7, activation='linear'))  # 7 features (adjust as necessary)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "    return model\n",
        "\n",
        "# Build the discriminator\n",
        "def build_discriminator(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output probability (real/fake)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the GAN (combining generator and discriminator)\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Freeze the discriminator during GAN training\n",
        "\n",
        "    # GAN input: random noise\n",
        "    gan_input = Input(shape=(latent_dim,))\n",
        "    x = generator(gan_input)\n",
        "\n",
        "    # The discriminator evaluates the generated sample\n",
        "    gan_output = discriminator(x)\n",
        "\n",
        "    # Create the GAN model\n",
        "    gan = Model(gan_input, gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "\n",
        "    return gan\n",
        "\n",
        "# Optimized training loop with smaller epochs and learning rate adjustment\n",
        "def train_gan(epochs, batch_size, latent_dim, X_train, generator, discriminator, gan):\n",
        "    batch_count = X_train.shape[0] // batch_size\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    # Ensure the loss variables are initialized\n",
        "    d_loss = 0\n",
        "    g_loss = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(batch_count):\n",
        "            # Train Discriminator\n",
        "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "            real_samples = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
        "            fake_samples = generator.predict(noise)\n",
        "\n",
        "            # Train discriminator on real and fake samples\n",
        "            d_loss_real = discriminator.train_on_batch(real_samples, np.ones((half_batch, 1)))\n",
        "            d_loss_fake = discriminator.train_on_batch(fake_samples, np.zeros((half_batch, 1)))\n",
        "\n",
        "            # Correctly calculate d_loss as an average of the real and fake losses\n",
        "            d_loss = 0.5 * np.add(d_loss_real[0], d_loss_fake[0])  # d_loss_real[0] and d_loss_fake[0] are loss values\n",
        "\n",
        "            # Train Generator (we want to trick the discriminator into classifying fake samples as real)\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))  # Label for generator is 1 (real)\n",
        "\n",
        "        # Learning rate scheduler (reducing the learning rate over time)\n",
        "        if epoch % 100 == 0:\n",
        "            current_lr = gan.optimizer.learning_rate\n",
        "            new_lr = current_lr * 0.95  # Reduce by 5% every 100 epochs\n",
        "            gan.optimizer.learning_rate.assign(new_lr)  # Update the learning rate\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} - Updated learning rate: {new_lr}\")\n",
        "\n",
        "        # Print training progress every few epochs (only after d_loss and g_loss are computed)\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'{epoch+1}/{epochs} | D Loss: {d_loss} | G Loss: {g_loss}')\n",
        "\n",
        "# Instantiate and compile the models\n",
        "generator = build_generator(latent_dim)\n",
        "discriminator = build_discriminator(normalized_data.shape[1])  # Input shape should match the data\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN with optimized settings\n",
        "epochs = 11    # Set the number of epochs to a smaller value for testing\n",
        "batch_size = 32  # Smaller batch size for faster training\n",
        "\n",
        "train_gan(epochs, batch_size, latent_dim, normalized_data, generator, discriminator, gan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVNn1m2BQ1wo",
        "outputId": "f4249c03-7275-4df5-9385-6d78fe56a01f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "[[-0.11528634 -0.14168109  0.08321266 ...  0.15021764  0.0142203\n",
            "  -0.17729372]\n",
            " [ 0.09401864 -0.18855149 -0.12699687 ...  0.18584234  0.05951953\n",
            "   0.04435223]\n",
            " [-0.1264144  -0.17500928 -0.1420508  ...  0.13162531 -0.01237944\n",
            "  -0.1169129 ]\n",
            " ...\n",
            " [-0.08167355 -0.2589823   0.09850308 ...  0.0763727   0.10813472\n",
            "  -0.17228699]\n",
            " [-0.0898655  -0.48946387  0.13655443 ... -0.18606772  0.1520841\n",
            "  -0.14812322]\n",
            " [-0.11362289 -0.20108548  0.10070263 ...  0.04314056 -0.03721219\n",
            "  -0.04978454]]\n"
          ]
        }
      ],
      "source": [
        "# Generate random noise (latent space vectors)\n",
        "num_samples = 1000  # The number of synthetic data samples you want to generate\n",
        "noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "\n",
        "# Generate synthetic samples using the trained generator\n",
        "synthetic_samples = generator.predict(noise)\n",
        "\n",
        "# Print the synthetic data generated\n",
        "print(synthetic_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTEYiZyQRaQV",
        "outputId": "0726c39b-d9bf-4ed2-b31c-d3beb9719455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 7)\n",
            "(1000, 7)\n"
          ]
        }
      ],
      "source": [
        "print(normalized_data.shape)  # Check the shape of your normalized training data\n",
        "print(synthetic_samples.shape)  # Check the shape of your generated samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "I83BXt9AR8oa",
        "outputId": "07ba4daa-2c14-4424-d361-ccf4b7fc2ee3"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_644dc8d1-9ec2-44dc-853d-d552c34aa973\", \"synthetic_soil_data.csv\", 83182)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Assuming 'synthetic_samples' is the generated data from the generator\n",
        "# Convert the synthetic data into a DataFrame\n",
        "synthetic_data_df = pd.DataFrame(synthetic_samples, columns=['Soil type','Water Holding Capacity', 'Field Capacity',\n",
        "                                                             'Permanent Wilting Point', 'Infiltration Rate',\n",
        "                                                             'Soil Texture Class', 'Bulk Density'])\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "synthetic_data_df.to_csv('synthetic_soil_data.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "files.download('synthetic_soil_data.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
