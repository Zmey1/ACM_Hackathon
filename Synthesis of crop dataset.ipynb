{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUvzWbhUfz1J",
        "outputId": "6244d25d-dcfa-48ec-9db0-73b8e7baeedf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow numpy pandas matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Crop List\n",
        "crops = ['Rice', 'Sugarcane', 'Banana', 'Cotton', 'Groundnut']\n",
        "\n",
        "# Create DataFrame to store the parameters\n",
        "data = {\n",
        "    'Crop': [],\n",
        "    'Kc_Germination': [],\n",
        "    'Kc_Initial': [],\n",
        "    'Kc_Mid_Season': [],\n",
        "    'Kc_Late_Season': [],\n",
        "    'Root_Depth_Germination': [],\n",
        "    'Root_Depth_Initial': [],\n",
        "    'Root_Depth_Mid_Season': [],\n",
        "    'Root_Depth_Late_Season': [],\n",
        "    'Critical_Depletion_Germination': [],\n",
        "    'Critical_Depletion_Initial': [],\n",
        "    'Critical_Depletion_Mid_Season': [],\n",
        "    'Critical_Depletion_Late_Season': [],\n",
        "    'Growing_Season_Start': [],\n",
        "    'Growing_Season_End': []\n",
        "}\n",
        "\n",
        "# Generate synthetic data (values are in expected ranges)\n",
        "for crop in crops:\n",
        "    data['Crop'].append(crop)\n",
        "    data['Kc_Germination'].append(np.random.uniform(0.3, 0.5))\n",
        "    data['Kc_Initial'].append(np.random.uniform(0.5, 0.7))\n",
        "    data['Kc_Mid_Season'].append(np.random.uniform(0.7, 1.2))\n",
        "    data['Kc_Late_Season'].append(np.random.uniform(0.6, 1.0))\n",
        "\n",
        "    data['Root_Depth_Germination'].append(np.random.uniform(10, 20))\n",
        "    data['Root_Depth_Initial'].append(np.random.uniform(20, 40))\n",
        "    data['Root_Depth_Mid_Season'].append(np.random.uniform(40, 70))\n",
        "    data['Root_Depth_Late_Season'].append(np.random.uniform(70, 100))\n",
        "\n",
        "    data['Critical_Depletion_Germination'].append(np.random.uniform(40, 60))\n",
        "    data['Critical_Depletion_Initial'].append(np.random.uniform(30, 50))\n",
        "    data['Critical_Depletion_Mid_Season'].append(np.random.uniform(20, 40))\n",
        "    data['Critical_Depletion_Late_Season'].append(np.random.uniform(10, 30))\n",
        "\n",
        "    data['Growing_Season_Start'].append('2023-06-01')  # This will depend on specific crop\n",
        "    data['Growing_Season_End'].append('2023-11-01')\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "gNm-fnoJgWAQ",
        "outputId": "6785fa1a-8d28-449a-84b1-954b6708e60a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Crop  Kc_Germination  Kc_Initial  Kc_Mid_Season  Kc_Late_Season  \\\n",
              "0       Rice        0.396946    0.690916       1.164972        0.827453   \n",
              "1  Sugarcane        0.370619    0.592813       0.791128        0.876357   \n",
              "2     Banana        0.463633    0.662877       1.113004        0.721839   \n",
              "3     Cotton        0.446258    0.516929       0.769907        0.985029   \n",
              "4  Groundnut        0.406150    0.501508       1.156499        0.604914   \n",
              "\n",
              "   Root_Depth_Germination  Root_Depth_Initial  Root_Depth_Mid_Season  \\\n",
              "0               18.171381           23.417834              46.606114   \n",
              "1               16.174263           29.956782              56.208394   \n",
              "2               11.333457           22.040979              62.448540   \n",
              "3               19.146186           35.292108              59.392564   \n",
              "4               13.578469           33.870163              47.870377   \n",
              "\n",
              "   Root_Depth_Late_Season  Critical_Depletion_Germination  \\\n",
              "0               93.900936                       57.229990   \n",
              "1               77.232105                       42.815670   \n",
              "2               86.148139                       51.902203   \n",
              "3               98.679670                       59.352283   \n",
              "4               96.377969                       58.358820   \n",
              "\n",
              "   Critical_Depletion_Initial  Critical_Depletion_Mid_Season  \\\n",
              "0                   31.406770                      25.273492   \n",
              "1                   49.866252                      32.775127   \n",
              "2                   39.151845                      37.627988   \n",
              "3                   43.646967                      33.077151   \n",
              "4                   45.499377                      21.577240   \n",
              "\n",
              "   Critical_Depletion_Late_Season Growing_Season_Start Growing_Season_End  \n",
              "0                       16.760016           2023-06-01         2023-11-01  \n",
              "1                       13.208487           2023-06-01         2023-11-01  \n",
              "2                       19.937238           2023-06-01         2023-11-01  \n",
              "3                       27.935243           2023-06-01         2023-11-01  \n",
              "4                       15.951032           2023-06-01         2023-11-01  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6504d66e-54bb-487d-b5d7-d0db0f9b249a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Crop</th>\n",
              "      <th>Kc_Germination</th>\n",
              "      <th>Kc_Initial</th>\n",
              "      <th>Kc_Mid_Season</th>\n",
              "      <th>Kc_Late_Season</th>\n",
              "      <th>Root_Depth_Germination</th>\n",
              "      <th>Root_Depth_Initial</th>\n",
              "      <th>Root_Depth_Mid_Season</th>\n",
              "      <th>Root_Depth_Late_Season</th>\n",
              "      <th>Critical_Depletion_Germination</th>\n",
              "      <th>Critical_Depletion_Initial</th>\n",
              "      <th>Critical_Depletion_Mid_Season</th>\n",
              "      <th>Critical_Depletion_Late_Season</th>\n",
              "      <th>Growing_Season_Start</th>\n",
              "      <th>Growing_Season_End</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rice</td>\n",
              "      <td>0.396946</td>\n",
              "      <td>0.690916</td>\n",
              "      <td>1.164972</td>\n",
              "      <td>0.827453</td>\n",
              "      <td>18.171381</td>\n",
              "      <td>23.417834</td>\n",
              "      <td>46.606114</td>\n",
              "      <td>93.900936</td>\n",
              "      <td>57.229990</td>\n",
              "      <td>31.406770</td>\n",
              "      <td>25.273492</td>\n",
              "      <td>16.760016</td>\n",
              "      <td>2023-06-01</td>\n",
              "      <td>2023-11-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sugarcane</td>\n",
              "      <td>0.370619</td>\n",
              "      <td>0.592813</td>\n",
              "      <td>0.791128</td>\n",
              "      <td>0.876357</td>\n",
              "      <td>16.174263</td>\n",
              "      <td>29.956782</td>\n",
              "      <td>56.208394</td>\n",
              "      <td>77.232105</td>\n",
              "      <td>42.815670</td>\n",
              "      <td>49.866252</td>\n",
              "      <td>32.775127</td>\n",
              "      <td>13.208487</td>\n",
              "      <td>2023-06-01</td>\n",
              "      <td>2023-11-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Banana</td>\n",
              "      <td>0.463633</td>\n",
              "      <td>0.662877</td>\n",
              "      <td>1.113004</td>\n",
              "      <td>0.721839</td>\n",
              "      <td>11.333457</td>\n",
              "      <td>22.040979</td>\n",
              "      <td>62.448540</td>\n",
              "      <td>86.148139</td>\n",
              "      <td>51.902203</td>\n",
              "      <td>39.151845</td>\n",
              "      <td>37.627988</td>\n",
              "      <td>19.937238</td>\n",
              "      <td>2023-06-01</td>\n",
              "      <td>2023-11-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cotton</td>\n",
              "      <td>0.446258</td>\n",
              "      <td>0.516929</td>\n",
              "      <td>0.769907</td>\n",
              "      <td>0.985029</td>\n",
              "      <td>19.146186</td>\n",
              "      <td>35.292108</td>\n",
              "      <td>59.392564</td>\n",
              "      <td>98.679670</td>\n",
              "      <td>59.352283</td>\n",
              "      <td>43.646967</td>\n",
              "      <td>33.077151</td>\n",
              "      <td>27.935243</td>\n",
              "      <td>2023-06-01</td>\n",
              "      <td>2023-11-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Groundnut</td>\n",
              "      <td>0.406150</td>\n",
              "      <td>0.501508</td>\n",
              "      <td>1.156499</td>\n",
              "      <td>0.604914</td>\n",
              "      <td>13.578469</td>\n",
              "      <td>33.870163</td>\n",
              "      <td>47.870377</td>\n",
              "      <td>96.377969</td>\n",
              "      <td>58.358820</td>\n",
              "      <td>45.499377</td>\n",
              "      <td>21.577240</td>\n",
              "      <td>15.951032</td>\n",
              "      <td>2023-06-01</td>\n",
              "      <td>2023-11-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6504d66e-54bb-487d-b5d7-d0db0f9b249a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6504d66e-54bb-487d-b5d7-d0db0f9b249a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6504d66e-54bb-487d-b5d7-d0db0f9b249a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-98ec8813-a622-45bf-a97b-975e689116ff\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-98ec8813-a622-45bf-a97b-975e689116ff')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-98ec8813-a622-45bf-a97b-975e689116ff button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Crop\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Sugarcane\",\n          \"Groundnut\",\n          \"Banana\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kc_Germination\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0377538705734221,\n        \"min\": 0.37061918808331873,\n        \"max\": 0.46363323148957786,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.37061918808331873,\n          0.40614964522870156,\n          0.46363323148957786\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kc_Initial\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08459884189916153,\n        \"min\": 0.5015083638570107,\n        \"max\": 0.6909162194024571,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.592812675848377,\n          0.5015083638570107,\n          0.6628765963575282\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kc_Mid_Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20065137343223421,\n        \"min\": 0.7699066850479275,\n        \"max\": 1.164971516412075,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7911281044631371,\n          1.156499067317802,\n          1.1130042727833587\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kc_Late_Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14572140154000496,\n        \"min\": 0.6049144291539087,\n        \"max\": 0.9850293928280132,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8763574990871894,\n          0.6049144291539087,\n          0.7218388600998122\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Root_Depth_Germination\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.231663159911187,\n        \"min\": 11.333456892491622,\n        \"max\": 19.146186002202835,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          16.174262883618866,\n          13.578469087445253,\n          11.333456892491622\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Root_Depth_Initial\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.995361674797985,\n        \"min\": 22.040979253274706,\n        \"max\": 35.29210840261505,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          29.956781951628823,\n          33.87016253275837,\n          22.040979253274706\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Root_Depth_Mid_Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.005360003806935,\n        \"min\": 46.60611374721298,\n        \"max\": 62.44853973377248,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          56.20839422100745,\n          47.870377430173725,\n          62.44853973377248\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Root_Depth_Late_Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.774885033348395,\n        \"min\": 77.2321047715212,\n        \"max\": 98.67966969171094,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          77.2321047715212,\n          96.37796900273864,\n          86.14813872623073\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Critical_Depletion_Germination\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.847368055470782,\n        \"min\": 42.81567033135488,\n        \"max\": 59.35228305808921,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          42.81567033135488,\n          58.35881974346469,\n          51.90220337080672\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Critical_Depletion_Initial\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.020110187703115,\n        \"min\": 31.406770178927594,\n        \"max\": 49.8662518483811,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          49.8662518483811,\n          45.499376502481724,\n          39.151844554683\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Critical_Depletion_Mid_Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.492635148088101,\n        \"min\": 21.57724016158586,\n        \"max\": 37.627988149624564,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          32.77512670755362,\n          21.57724016158586,\n          37.627988149624564\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Critical_Depletion_Late_Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.663049126317488,\n        \"min\": 13.20848688093581,\n        \"max\": 27.93524330384956,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          13.20848688093581,\n          15.951031510223801,\n          19.937237565079812\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Growing_Season_Start\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2023-06-01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Growing_Season_End\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2023-11-01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_generator():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, input_dim=100, activation='relu'))\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(13, activation='linear'))  # 13 output features\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "rV3OcksvgdQP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(512, input_dim=13, activation='relu'))\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # Output: real or fake\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "MJF1oDLtghLA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = models.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "OIP4-6USgkdJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate numeric and non-numeric columns\n",
        "numeric_columns = ['Kc_Germination', 'Kc_Initial', 'Kc_Mid_Season', 'Kc_Late_Season',\n",
        "                   'Root_Depth_Germination', 'Root_Depth_Initial', 'Root_Depth_Mid_Season',\n",
        "                   'Root_Depth_Late_Season', 'Critical_Depletion_Germination',\n",
        "                   'Critical_Depletion_Initial', 'Critical_Depletion_Mid_Season',\n",
        "                   'Critical_Depletion_Late_Season']\n",
        "\n",
        "non_numeric_columns = ['Crop', 'Growing_Season_Start', 'Growing_Season_End']\n",
        "\n",
        "# Now, you can separate the numeric and non-numeric data\n",
        "df_numeric = df[numeric_columns].values.astype(np.float32)\n",
        "df_non_numeric = df[non_numeric_columns]\n",
        "\n",
        "# Check the shapes\n",
        "print(df_numeric.shape)  # Should contain only numeric values\n",
        "print(df_non_numeric.head())  # Should contain only non-numeric values\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15R-V3K2hO_q",
        "outputId": "058c1c8c-796f-4893-ac5e-a308ac214aa1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 12)\n",
            "        Crop Growing_Season_Start Growing_Season_End\n",
            "0       Rice           2023-06-01         2023-11-01\n",
            "1  Sugarcane           2023-06-01         2023-11-01\n",
            "2     Banana           2023-06-01         2023-11-01\n",
            "3     Cotton           2023-06-01         2023-11-01\n",
            "4  Groundnut           2023-06-01         2023-11-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with numeric data\n",
        "for epoch in range(epochs):\n",
        "    # Train Discriminator\n",
        "    idx = np.random.randint(0, df_numeric.shape[0], batch_size)\n",
        "    real_data = df_numeric[idx]  # Now using the numeric data\n",
        "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "    fake_data = generator.predict(noise)\n",
        "\n",
        "    # Label smoothing (real = 0.9 to help stabilize training)\n",
        "    real_labels = np.ones((batch_size, 1)) * 0.9\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train Generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "    valid_labels = np.ones((batch_size, 1))\n",
        "    g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "    if epoch % 1 == 0:  # Log the loss for every epoch\n",
        "        print(f\"{epoch+1} [D loss: {d_loss[0]}] [G loss: {g_loss}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "zFSbDaV_hrsY",
        "outputId": "df5bbc76-8c8b-45c7-e28f-e048e668e816"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_4\" is incompatible with the layer: expected axis -1 of input shape to have value 13, but received input with shape (32, 12)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 12), dtype=float32)\n  • training=True\n  • mask=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a537c4e2399c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 ):\n\u001b[0;32m--> 227\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mone_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             outputs = reduce_per_replica(\n\u001b[1;32m    115\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_has_training_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 }:\n\u001b[0;32m--> 227\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    228\u001b[0m                         \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;34mf\"incompatible with the layer: expected axis {axis} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_4\" is incompatible with the layer: expected axis -1 of input shape to have value 13, but received input with shape (32, 12)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 12), dtype=float32)\n  • training=True\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_dim=z_dim))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dense(12, activation='tanh'))  # 12 output features for numeric data\n",
        "    return model\n",
        "\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(1024, activation='relu', input_dim=12))  # Expect 12 features\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "qOndabUIh9d8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the Crop column\n",
        "crop_one_hot = pd.get_dummies(df['Crop'])\n",
        "\n",
        "# Convert df_numeric (NumPy array) to DataFrame\n",
        "df_numeric_df = pd.DataFrame(df_numeric, columns=numeric_columns)\n",
        "\n",
        "# Combine the one-hot encoded crop data with the numeric data\n",
        "df_with_crop = pd.concat([df_numeric_df, crop_one_hot], axis=1)\n",
        "\n",
        "# Check the shape of the new data (should now have 13 columns)\n",
        "print(df_with_crop.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b5wa4tciMEO",
        "outputId": "e67cd713-daa5-48be-bf76-dbbfadac8a77"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, create the models by calling the functions\n",
        "discriminator = build_discriminator()  # Instantiate the discriminator model\n",
        "generator = build_generator()  # Instantiate the generator model\n",
        "\n",
        "# Compile the discriminator\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Create the GAN model (stack the generator and discriminator)\n",
        "discriminator.trainable = False  # We don't want to train the discriminator when training the generator\n",
        "gan = build_gan(generator, discriminator)  # The GAN model that combines the generator and discriminator\n",
        "\n",
        "# Now you can proceed with the training loop using the instantiated models\n",
        "for epoch in range(epochs):\n",
        "    # Train Discriminator\n",
        "    idx = np.random.randint(0, df_with_crop.shape[0], batch_size)\n",
        "    real_data = df_with_crop.iloc[idx].values  # Get the real data\n",
        "    noise = np.random.normal(0, 1, (batch_size, z_dim))  # Generate random noise for the generator\n",
        "    fake_data = generator.predict(noise)  # Generate fake data using the generator\n",
        "\n",
        "    # Label smoothing (real = 0.9 to help stabilize training)\n",
        "    real_labels = np.ones((batch_size, 1)) * 0.9\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train Generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, z_dim))  # Generate noise for generator input\n",
        "    valid_labels = np.ones((batch_size, 1))  # We want the generator to fool the discriminator\n",
        "    g_loss = gan.train_on_batch(noise, valid_labels)  # Train the GAN by updating the generator\n",
        "\n",
        "    if epoch % 1 == 0:  # Log the loss for every epoch\n",
        "        print(f\"{epoch+1} [D loss: {d_loss[0]}] [G loss: {g_loss}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "hVDly5FRilR8",
        "outputId": "0f04f496-da21-4779-e46e-2a006e1f2e88"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Dense' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d660920a588a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First, create the models by calling the functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Instantiate the discriminator model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Instantiate the generator model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compile the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d1439ba6ffb3>\u001b[0m in \u001b[0;36mbuild_discriminator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expect 12 features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dense' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
        "\n",
        "# Function to build the generator model\n",
        "def build_generator(z_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_dim=z_dim))  # Input is the random noise (latent vector)\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dense(12, activation='tanh'))  # Output layer: 12 features for the generated data\n",
        "    return model\n",
        "\n",
        "# Function to build the discriminator model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, activation='relu', input_dim=12))  # Expect 12 features as input\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification (real or fake)\n",
        "    return model\n",
        "\n",
        "# Function to build the GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "iASd7ClRizHh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the models\n",
        "discriminator = build_discriminator()\n",
        "generator = build_generator(z_dim=100)  # Define your z_dim (latent vector size)\n",
        "\n",
        "# Compile the discriminator model\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Create the GAN model by stacking the generator and discriminator\n",
        "discriminator.trainable = False  # Freeze the discriminator during generator training\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Compile the GAN model\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrvBfMWgi1f6",
        "outputId": "662a6062-8b06-4715-bb79-b13def9542f3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 1: Load your data\n",
        "# Assuming df is your dataframe containing both numeric and non-numeric (categorical) columns\n",
        "# Example structure: df has 'Crop', 'SoilType', 'Month', etc.\n",
        "\n",
        "# Example of one possible structure\n",
        "# df = pd.DataFrame({\n",
        "#     'Crop': ['rice', 'sugarcane', 'banana', 'rice', 'sugarcane'],\n",
        "#     'SoilType': ['red loamy', 'black cotton', 'red loamy', 'black cotton', 'red loamy'],\n",
        "#     'Kc_stage1': [0.8, 1.0, 0.7, 0.8, 1.0],\n",
        "#     'Kc_stage2': [1.1, 1.2, 1.0, 1.1, 1.2],\n",
        "#     'Root_depth_stage1': [50, 60, 45, 50, 60],\n",
        "#     'Root_depth_stage2': [75, 85, 70, 75, 85],\n",
        "#     'Critical_depletion': [30, 40, 35, 30, 40]\n",
        "# })\n",
        "\n",
        "# Step 2: One-hot encode the categorical columns\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "categorical_columns = ['Crop', 'SoilType']  # Define your categorical columns\n",
        "\n",
        "# Perform one-hot encoding and get a DataFrame\n",
        "encoded_categories = encoder.fit_transform(df[categorical_columns])\n",
        "encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Step 3: Combine one-hot encoded data with numeric columns\n",
        "df_numeric = df.drop(columns=categorical_columns)  # Drop categorical columns from original df\n",
        "df_combined = pd.concat([df_numeric, encoded_df], axis=1)  # Combine numeric and encoded categorical data\n",
        "\n",
        "# Step 4: Ensure all data is in float32 for TensorFlow compatibility\n",
        "df_combined = df_combined.astype(np.float32)\n",
        "\n",
        "# Step 5: Define the model architecture (same as before but adjusted for this new data)\n",
        "# Assuming the shape of the combined data is (batch_size, 12)\n",
        "\n",
        "# Build the Discriminator\n",
        "def build_discriminator(input_shape=(12,)):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # Output layer (real or fake)\n",
        "    return model\n",
        "\n",
        "# Build the Generator\n",
        "def build_generator(z_dim=100):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu', input_dim=z_dim))\n",
        "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(12, activation='tanh'))  # Output layer matching the input shape (12 features)\n",
        "    return model\n",
        "\n",
        "# Build the GAN\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Freeze discriminator when training GAN\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n",
        "# Instantiate models\n",
        "discriminator = build_discriminator(input_shape=(df_combined.shape[1],))\n",
        "generator = build_generator(z_dim=100)\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Compile the models\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), metrics=['accuracy'])\n",
        "gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002))\n",
        "\n",
        "# Step 6: Training Loop\n",
        "epochs = 11  # Number of epochs\n",
        "batch_size = 32\n",
        "z_dim = 100  # Latent vector size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Train the Discriminator\n",
        "    idx = np.random.randint(0, df_combined.shape[0], batch_size)\n",
        "    real_data = df_combined.iloc[idx].values  # Get a batch of real data\n",
        "\n",
        "    # Generate fake data\n",
        "    noise = np.random.normal(0, 1, (batch_size, z_dim))  # Generate random noise\n",
        "    fake_data = generator.predict(noise)  # Generate fake data using the generator\n",
        "\n",
        "    # Labels: real = 1, fake = 0\n",
        "    real_labels = np.ones((batch_size, 1)) * 0.9  # Slight label smoothing for stability\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the discriminator on real and fake data\n",
        "    d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)  # Average the losses\n",
        "\n",
        "    # Train the Generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "    valid_labels = np.ones((batch_size, 1))  # Generator wants to fool the discriminator\n",
        "    g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "    # Print losses for every epoch\n",
        "    if epoch % 1 == 0:\n",
        "        print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]}] [G loss: {g_loss}]\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "dTpSWM2Tj5Zy",
        "outputId": "0805a1aa-8791-4369-e382-4f5250f87fae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e8f4f12cec18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Step 2: One-hot encode the categorical columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mcategorical_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Crop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SoilType'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Define your categorical columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 2: One-hot encode the categorical columns\n",
        "encoder = OneHotEncoder(sparse_output=False)  # Updated argument name\n",
        "categorical_columns = ['Crop', 'SoilType']  # Define your categorical columns\n",
        "\n",
        "# Perform one-hot encoding and get a DataFrame\n",
        "encoded_categories = encoder.fit_transform(df[categorical_columns])\n",
        "encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Step 3: Combine one-hot encoded data with numeric columns\n",
        "df_numeric = df.drop(columns=categorical_columns)  # Drop categorical columns from original df\n",
        "df_combined = pd.concat([df_numeric, encoded_df], axis=1)  # Combine numeric and encoded categorical data\n",
        "\n",
        "# Step 4: Ensure all data is in float32 for TensorFlow compatibility\n",
        "df_combined = df_combined.astype(np.float32)\n",
        "\n",
        "# Proceed with the rest of your code...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "HECnJ0k5kEAA",
        "outputId": "2a6cfb57-bdaf-40b0-f3c9-42a99655cad1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['SoilType'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a05b411f9c77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Perform one-hot encoding and get a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mencoded_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mencoded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['SoilType'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your DataFrame (replace this with your actual df loading code)\n",
        "# df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Check if the column names are correct\n",
        "print(df.columns)  # This will show all the column names in df\n",
        "\n",
        "# For example, if 'SoilType' had a typo, fix it by using the correct name\n",
        "categorical_columns = ['Crop', 'SoilType']  # Ensure these are valid columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL8Fq3IYkMR6",
        "outputId": "18141fd2-24ac-4d05-f1dc-da81f9c77e31"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Crop', 'Kc_Germination', 'Kc_Initial', 'Kc_Mid_Season',\n",
            "       'Kc_Late_Season', 'Root_Depth_Germination', 'Root_Depth_Initial',\n",
            "       'Root_Depth_Mid_Season', 'Root_Depth_Late_Season',\n",
            "       'Critical_Depletion_Germination', 'Critical_Depletion_Initial',\n",
            "       'Critical_Depletion_Mid_Season', 'Critical_Depletion_Late_Season',\n",
            "       'Growing_Season_Start', 'Growing_Season_End'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 2: One-hot encode the categorical columns\n",
        "encoder = OneHotEncoder(sparse_output=False)  # Updated argument name\n",
        "categorical_columns = ['Crop', 'SoilType']  # Define your categorical columns\n",
        "\n",
        "# Perform one-hot encoding and get a DataFrame\n",
        "encoded_categories = encoder.fit_transform(df[categorical_columns])\n",
        "encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Step 3: Combine one-hot encoded data with numeric columns\n",
        "df_numeric = df.drop(columns=categorical_columns)  # Drop categorical columns from original df\n",
        "df_combined = pd.concat([df_numeric, encoded_df], axis=1)  # Combine numeric and encoded categorical data\n",
        "\n",
        "# Step 4: Ensure all data is in float32 for TensorFlow compatibility\n",
        "df_combined = df_combined.astype(np.float32)\n",
        "\n",
        "# Proceed with the rest of your code...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "fgGPN2swkQ5U",
        "outputId": "5d92f819-8df1-4161-b7d2-7d5443d4626c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['SoilType'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a05b411f9c77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Perform one-hot encoding and get a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mencoded_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mencoded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['SoilType'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()  # Strip any leading/trailing spaces\n",
        "print(df.columns)  # Check if 'SoilType' is now listed correctly\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p02UVGK7kZgN",
        "outputId": "83b88245-29e7-4112-e955-d575767526c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Crop', 'Kc_Germination', 'Kc_Initial', 'Kc_Mid_Season',\n",
            "       'Kc_Late_Season', 'Root_Depth_Germination', 'Root_Depth_Initial',\n",
            "       'Root_Depth_Mid_Season', 'Root_Depth_Late_Season',\n",
            "       'Critical_Depletion_Germination', 'Critical_Depletion_Initial',\n",
            "       'Critical_Depletion_Mid_Season', 'Critical_Depletion_Late_Season',\n",
            "       'Growing_Season_Start', 'Growing_Season_End'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Check the columns of the DataFrame\n",
        "print(\"Columns in the DataFrame:\", df.columns)  # This will help identify the issue\n",
        "\n",
        "# Strip any extra spaces from the column names to avoid hidden issues\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Check again if 'SoilType' and 'Crop' exist in the DataFrame\n",
        "print(\"Columns after stripping spaces:\", df.columns)\n",
        "\n",
        "# Ensure the correct columns are selected for one-hot encoding\n",
        "categorical_columns = ['Crop', 'SoilType']  # Ensure these are valid columns\n",
        "\n",
        "# If 'SoilType' is not found, check if there is a typo or other column name\n",
        "missing_columns = [col for col in categorical_columns if col not in df.columns]\n",
        "if missing_columns:\n",
        "    print(f\"Missing columns: {missing_columns}\")\n",
        "else:\n",
        "    # Perform one-hot encoding if columns exist\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    encoded_categories = encoder.fit_transform(df[categorical_columns])\n",
        "\n",
        "    # Create a DataFrame from the one-hot encoded categories\n",
        "    encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "    # Step 3: Combine one-hot encoded data with numeric columns\n",
        "    df_numeric = df.drop(columns=categorical_columns)  # Drop categorical columns from original df\n",
        "    df_combined = pd.concat([df_numeric, encoded_df], axis=1)  # Combine numeric and encoded categorical data\n",
        "\n",
        "    # Step 4: Ensure all data is in float32 for TensorFlow compatibility\n",
        "    df_combined = df_combined.astype(np.float32)\n",
        "\n",
        "    print(\"Data preprocessing complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGYhzNTFkcns",
        "outputId": "4698c2e8-5ae9-4053-cf4e-e658a59ae841"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the DataFrame: Index(['Crop', 'Kc_Germination', 'Kc_Initial', 'Kc_Mid_Season',\n",
            "       'Kc_Late_Season', 'Root_Depth_Germination', 'Root_Depth_Initial',\n",
            "       'Root_Depth_Mid_Season', 'Root_Depth_Late_Season',\n",
            "       'Critical_Depletion_Germination', 'Critical_Depletion_Initial',\n",
            "       'Critical_Depletion_Mid_Season', 'Critical_Depletion_Late_Season',\n",
            "       'Growing_Season_Start', 'Growing_Season_End'],\n",
            "      dtype='object')\n",
            "Columns after stripping spaces: Index(['Crop', 'Kc_Germination', 'Kc_Initial', 'Kc_Mid_Season',\n",
            "       'Kc_Late_Season', 'Root_Depth_Germination', 'Root_Depth_Initial',\n",
            "       'Root_Depth_Mid_Season', 'Root_Depth_Late_Season',\n",
            "       'Critical_Depletion_Germination', 'Critical_Depletion_Initial',\n",
            "       'Critical_Depletion_Mid_Season', 'Critical_Depletion_Late_Season',\n",
            "       'Growing_Season_Start', 'Growing_Season_End'],\n",
            "      dtype='object')\n",
            "Missing columns: ['SoilType']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary columns if missing (for testing purposes)\n",
        "if 'SoilType' not in df.columns:\n",
        "    df['SoilType'] = 'Red Loamy'  # Example, replace with valid data\n",
        "\n",
        "if 'Crop' not in df.columns:\n",
        "    df['Crop'] = 'Rice'  # Example, replace with valid data\n"
      ],
      "metadata": {
        "id": "ycSPV2kokmi1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 2: One-hot encode the categorical columns\n",
        "encoder = OneHotEncoder(sparse_output=False)  # Updated argument name\n",
        "categorical_columns = ['Crop', 'SoilType']  # Define your categorical columns\n",
        "\n",
        "# Perform one-hot encoding and get a DataFrame\n",
        "encoded_categories = encoder.fit_transform(df[categorical_columns])\n",
        "encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Step 3: Combine one-hot encoded data with numeric columns\n",
        "df_numeric = df.drop(columns=categorical_columns)  # Drop categorical columns from original df\n",
        "df_combined = pd.concat([df_numeric, encoded_df], axis=1)  # Combine numeric and encoded categorical data\n",
        "\n",
        "# Step 4: Ensure all data is in float32 for TensorFlow compatibility\n",
        "df_combined = df_combined.astype(np.float32)\n",
        "\n",
        "# Proceed with the rest of your code...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "MT7GOSUvkrKB",
        "outputId": "c85b4f0e-4f0d-4fc8-b54e-307d8e80dfb0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '2023-06-01'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a05b411f9c77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Step 4: Ensure all data is in float32 for TensorFlow compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Proceed with the rest of your code...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_astype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2023-06-01'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date column to datetime type\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# If you want to convert the date into a numerical value (e.g., days since a reference date)\n",
        "df['Date'] = (df['Date'] - pd.to_datetime('1970-01-01')).dt.total_seconds()\n",
        "\n",
        "# Now you can drop the 'Date' column if you don't need it or keep it for modeling.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "uKpOs9Qck5JE",
        "outputId": "90a16075-0e9e-46ae-d9ed-da4223261fa6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1b394d429335>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert date column to datetime type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# If you want to convert the date into a numerical value (e.g., days since a reference date)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1970-01-01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Check column names and data types\n",
        "print(df.columns)\n",
        "print(df.dtypes)\n",
        "\n",
        "# Example of handling date columns\n",
        "# If there is a column containing dates (e.g., 'PlantingDate', 'HarvestDate', etc.)\n",
        "# Replace 'PlantingDate' with your actual column name if it's different\n",
        "date_columns = ['PlantingDate']  # List any date columns in your dataset\n",
        "\n",
        "for date_col in date_columns:\n",
        "    if date_col in df.columns:\n",
        "        # Convert the column to datetime\n",
        "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "\n",
        "        # Convert the date to a numerical value (number of seconds since Unix epoch)\n",
        "        df[date_col] = (df[date_col] - pd.to_datetime('1970-01-01')).dt.total_seconds()\n",
        "\n",
        "# Perform one-hot encoding on categorical columns\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "categorical_columns = ['Crop', 'SoilType']  # Adjust based on your actual categorical columns\n",
        "\n",
        "# Perform one-hot encoding and get a DataFrame\n",
        "encoded_categories = encoder.fit_transform(df[categorical_columns])\n",
        "encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Combine one-hot encoded data with numeric columns\n",
        "df_numeric = df.drop(columns=categorical_columns)  # Drop categorical columns\n",
        "df_combined = pd.concat([df_numeric, encoded_df], axis=1)  # Combine numeric and encoded data\n",
        "\n",
        "# Convert numeric columns to float32\n",
        "df_combined = df_combined.apply(pd.to_numeric, errors='ignore')  # Convert all numeric columns\n",
        "df_combined = df_combined.astype(np.float32, errors='ignore')  # Ensure the correct type for numeric data\n",
        "\n",
        "# Now df_combined is ready for model training or further analysis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbi6V-eSlFyJ",
        "outputId": "a68bfb33-cd97-4c1a-edfa-6fef189dc5a2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Crop', 'Kc_Germination', 'Kc_Initial', 'Kc_Mid_Season',\n",
            "       'Kc_Late_Season', 'Root_Depth_Germination', 'Root_Depth_Initial',\n",
            "       'Root_Depth_Mid_Season', 'Root_Depth_Late_Season',\n",
            "       'Critical_Depletion_Germination', 'Critical_Depletion_Initial',\n",
            "       'Critical_Depletion_Mid_Season', 'Critical_Depletion_Late_Season',\n",
            "       'Growing_Season_Start', 'Growing_Season_End', 'SoilType'],\n",
            "      dtype='object')\n",
            "Crop                               object\n",
            "Kc_Germination                    float64\n",
            "Kc_Initial                        float64\n",
            "Kc_Mid_Season                     float64\n",
            "Kc_Late_Season                    float64\n",
            "Root_Depth_Germination            float64\n",
            "Root_Depth_Initial                float64\n",
            "Root_Depth_Mid_Season             float64\n",
            "Root_Depth_Late_Season            float64\n",
            "Critical_Depletion_Germination    float64\n",
            "Critical_Depletion_Initial        float64\n",
            "Critical_Depletion_Mid_Season     float64\n",
            "Critical_Depletion_Late_Season    float64\n",
            "Growing_Season_Start               object\n",
            "Growing_Season_End                 object\n",
            "SoilType                           object\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-4955cf1bbe44>:34: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  df_combined = df_combined.apply(pd.to_numeric, errors='ignore')  # Convert all numeric columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxiqoBTJlxnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Discriminator model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout\n",
        "\n",
        "def build_discriminator(input_dim=12):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, activation=LeakyReLU(alpha=0.2), input_dim=input_dim))\n",
        "    model.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
        "    model.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification (real/fake)\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator(input_dim=12)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8oTYiFJlubR",
        "outputId": "68567879-2b57-4388-a2d8-910480f2b267"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Generator model\n",
        "from tensorflow.keras.layers import Dense, Reshape, BatchNormalization\n",
        "\n",
        "def build_generator(z_dim=100, output_dim=12):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, activation=LeakyReLU(alpha=0.2), input_dim=z_dim))\n",
        "    model.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
        "    model.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\n",
        "    model.add(Dense(output_dim, activation='tanh'))  # Output layer for 12 features\n",
        "    return model\n",
        "\n",
        "generator = build_generator(z_dim=100, output_dim=12)\n"
      ],
      "metadata": {
        "id": "aIEk42ZrlzdC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GAN model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Freeze the discriminator during GAN training\n",
        "    z = Input(shape=(100,))  # Latent space input\n",
        "    generated_data = generator(z)\n",
        "    validity = discriminator(generated_data)  # Discriminator's output\n",
        "    model = Model(z, validity)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "gan = build_gan(generator, discriminator)\n"
      ],
      "metadata": {
        "id": "vE18hTZ_l2KF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Discriminator model with the correct input dimension\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU\n",
        "\n",
        "def build_discriminator(input_dim=12):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, activation=LeakyReLU(alpha=0.2), input_dim=input_dim))\n",
        "    model.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
        "    model.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification (real/fake)\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator(input_dim=12)  # Ensure it's 12 features\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "yIqH70_nmb66"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have 12 numeric features and additional one-hot encoded columns, count the total number of columns\n",
        "input_dim = len(df_combined.columns)  # Update this based on your final number of features\n",
        "\n",
        "# Update the discriminator with the correct input dimension\n",
        "discriminator = build_discriminator(input_dim=input_dim)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "A_x5ErUkmuNt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Select a random batch of real data\n",
        "    idx = np.random.randint(0, real_data.shape[0], half_batch)\n",
        "    real_batch = real_data[idx]\n",
        "\n",
        "    # Check if the shape of real_batch is correct\n",
        "    print(f\"Shape of real_batch: {real_batch.shape}\")  # This should be (half_batch, 12)\n",
        "\n",
        "    # Generate a batch of fake data using the generator\n",
        "    noise = np.random.normal(0, 1, (half_batch, z_dim))  # Latent space\n",
        "    fake_batch = generator.predict(noise)\n",
        "\n",
        "    # Check if the shape of fake_batch is correct\n",
        "    print(f\"Shape of fake_batch: {fake_batch.shape}\")  # This should be (half_batch, 12)\n",
        "\n",
        "    # Train the discriminator on real and fake data\n",
        "    d_loss_real = discriminator.train_on_batch(real_batch, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_batch, fake_labels)\n",
        "\n",
        "    # Average discriminator loss\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, z_dim))  # Latent space\n",
        "    g_loss = gan.train_on_batch(noise, real_labels)  # Use real labels to fool the discriminator\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, D accuracy: {d_loss[1]*100:.2f}%] [G loss: {g_loss:.4f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "tR7jJM1kmyMt",
        "outputId": "a5b3f7cb-d02a-4347-c1f3-b7b6adb78379"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of real_batch: (32, 20)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Shape of fake_batch: (32, 12)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-56b6e2eb1d70>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Train the discriminator on real and fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 ):\n\u001b[0;32m--> 227\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36munpack_inputs\u001b[0;34m(self, bound_parameters)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m       flat.extend(\n\u001b[0;32m--> 391\u001b[0;31m           \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_constraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m       )\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/trace_type/default_types.py\u001b[0m in \u001b[0;36mto_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mflattened_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcomp_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0mflattened_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mflattened_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Example dataframe (replace with your own dataframe)\n",
        "df = pd.DataFrame({\n",
        "    'Crop': ['Rice', 'Banana', 'Sugarcane', 'Rice'],\n",
        "    'SoilType': ['Red Loamy', 'Black Cotton', 'Red Loamy', 'Black Cotton'],\n",
        "    'Date': ['2023-06-01', '2023-06-02', '2023-06-03', '2023-06-04'],\n",
        "    'Irrigation': [200, 250, 180, 210],\n",
        "    'Rainfall': [120, 110, 130, 140]\n",
        "})\n",
        "\n",
        "# Define the categorical columns\n",
        "categorical_columns = ['Crop', 'SoilType']\n",
        "\n",
        "# Apply OneHotEncoding\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_categories = encoder.fit_transform(df[categorical_columns])\n",
        "\n",
        "# Convert the one-hot encoded columns into a dataframe\n",
        "encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Drop original categorical columns\n",
        "df_combined = pd.concat([df.drop(columns=categorical_columns), encoded_df], axis=1)\n",
        "\n",
        "# Check the final dataframe\n",
        "print(df_combined.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "PiF4OouMnDeY",
        "outputId": "f309002e-553e-48ed-fbd1-b05e1182b93a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-adb33846b156>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Apply OneHotEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mencoded_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataframe (replace with your own dataframe)\n",
        "df = pd.DataFrame({\n",
        "    'Crop': ['Rice', 'Banana', 'Sugarcane', 'Rice'],\n",
        "    'SoilType': ['Red Loamy', 'Black Cotton', 'Red Loamy', 'Black Cotton'],\n",
        "    'Date': ['2023-06-01', '2023-06-02', '2023-06-03', '2023-06-04'],\n",
        "    'Irrigation': [200, 250, 180, 210],\n",
        "    'Rainfall': [120, 110, 130, 140]\n",
        "})\n",
        "\n",
        "# Define the categorical columns\n",
        "categorical_columns = ['Crop', 'SoilType']\n",
        "\n",
        "# Apply OneHotEncoding (corrected)\n",
        "encoder = OneHotEncoder(sparse_output=False)  # sparse=False is replaced by sparse_output=False\n",
        "encoded_categories = encoder.fit_transform(df[categorical_columns])\n",
        "\n",
        "# Convert the one-hot encoded columns into a dataframe\n",
        "encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Drop original categorical columns\n",
        "df_combined = pd.concat([df.drop(columns=categorical_columns), encoded_df], axis=1)\n",
        "\n",
        "# Check the final dataframe\n",
        "print(df_combined.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-C3gjmdnNyU",
        "outputId": "499264f3-655b-40b7-c7e0-7e6ee4458661"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date  Irrigation  Rainfall  Crop_Banana  Crop_Rice  Crop_Sugarcane  \\\n",
            "0  2023-06-01         200       120          0.0        1.0             0.0   \n",
            "1  2023-06-02         250       110          1.0        0.0             0.0   \n",
            "2  2023-06-03         180       130          0.0        0.0             1.0   \n",
            "3  2023-06-04         210       140          0.0        1.0             0.0   \n",
            "\n",
            "   SoilType_Black Cotton  SoilType_Red Loamy  \n",
            "0                    0.0                 1.0  \n",
            "1                    1.0                 0.0  \n",
            "2                    0.0                 1.0  \n",
            "3                    1.0                 0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "HYpxVnicnWi7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation=LeakyReLU(0.2)))\n",
        "    model.add(Dense(64, activation=LeakyReLU(0.2)))\n",
        "    model.add(Dense(32, activation=LeakyReLU(0.2)))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output layer to classify real or fake\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_generator(latent_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=latent_dim, activation=LeakyReLU(0.2)))\n",
        "    model.add(Dense(128, activation=LeakyReLU(0.2)))\n",
        "    model.add(Dense(256, activation=LeakyReLU(0.2)))\n",
        "    model.add(Dense(output_dim, activation='tanh'))  # Output layer matches the data dimension\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ItwYBlsMnZIQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    # Freeze the discriminator during GAN training\n",
        "    discriminator.trainable = False\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "sLab-4iXncCU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim):\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train the discriminator\n",
        "\n",
        "        # Select a random half batch of real data\n",
        "        real_batch = df_combined.sample(n=half_batch)\n",
        "        real_labels = np.ones((half_batch, 1))  # Labels for real data\n",
        "\n",
        "        # Generate fake data\n",
        "        noise = np.random.normal(0, 1, (half_batch, latent_dim))  # Latent space noise\n",
        "        fake_batch = generator.predict(noise)\n",
        "        fake_labels = np.zeros((half_batch, 1))  # Labels for fake data\n",
        "\n",
        "        # Train the discriminator on real and fake data\n",
        "        d_loss_real = discriminator.train_on_batch(real_batch, real_labels)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_batch, fake_labels)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))  # Latent space noise\n",
        "        valid_labels = np.ones((batch_size, 1))  # Labels for generator to fool the discriminator\n",
        "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "        # Print the progress\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch} | D Loss: {d_loss[0]} | G Loss: {g_loss}\")\n"
      ],
      "metadata": {
        "id": "0_S7XZG9nhCy"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "latent_dim = 100  # Dimensionality of the random noise\n",
        "output_dim = df_combined.shape[1]  # Number of features in the data\n",
        "\n",
        "# Build and compile models\n",
        "discriminator = build_discriminator(output_dim)\n",
        "generator = build_generator(latent_dim, output_dim)\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN for 11 epochs\n",
        "epochs = 11\n",
        "batch_size = 32\n",
        "train_gan(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "2vzZvIYMn2r3",
        "outputId": "9b1617fc-040b-44f5-f77b-c2c5a48f7488"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot take a larger sample than population when 'replace=False'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-c5d740ff6793>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-831171518629>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Select a random half batch of real data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mreal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mreal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Labels for real data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6116\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6118\u001b[0;31m         \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6119\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/sample.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid weights: weights sum to zero\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     )\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the training loop for GAN\n",
        "def train_gan(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim):\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Ensure batch_size is not greater than dataset size\n",
        "        if batch_size > df_combined.shape[0]:\n",
        "            batch_size = df_combined.shape[0]\n",
        "\n",
        "        # Sample a random half batch of real data\n",
        "        real_batch = df_combined.sample(n=half_batch, replace=True)  # Use replace=True to avoid sampling errors\n",
        "        real_labels = np.ones((half_batch, 1))  # Labels for real data (1)\n",
        "\n",
        "        # Generate fake data\n",
        "        noise = np.random.normal(0, 1, (half_batch, latent_dim))  # Latent space noise for generating fake data\n",
        "        fake_batch = generator.predict(noise)\n",
        "        fake_labels = np.zeros((half_batch, 1))  # Labels for fake data (0)\n",
        "\n",
        "        # Train the discriminator on real and fake data\n",
        "        d_loss_real = discriminator.train_on_batch(real_batch, real_labels)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_batch, fake_labels)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))  # Latent space noise for the generator\n",
        "        valid_labels = np.ones((batch_size, 1))  # Labels for the generator to fool the discriminator\n",
        "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "        # Print the progress every epoch\n",
        "        if epoch % 1 == 0:\n",
        "            print(f\"Epoch {epoch} | D Loss: {d_loss[0]} | G Loss: {g_loss}\")\n",
        "\n",
        "# Set parameters for the GAN model\n",
        "latent_dim = 100  # Dimensionality of the random noise\n",
        "output_dim = df_combined.shape[1]  # Number of features in the data\n",
        "\n",
        "# Build and compile the models\n",
        "discriminator = build_discriminator(output_dim)\n",
        "generator = build_generator(latent_dim, output_dim)\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN for 11 epochs\n",
        "epochs = 11\n",
        "batch_size = 32\n",
        "train_gan(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "tr3XuDHsodHM",
        "outputId": "e7f285b2-d600-4700-8883-8e04f7df7e04"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bab006d1120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-e08f1dcc0aa8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-e08f1dcc0aa8>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Train the discriminator on real and fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 ):\n\u001b[0;32m--> 227\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36munpack_inputs\u001b[0;34m(self, bound_parameters)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m       flat.extend(\n\u001b[0;32m--> 391\u001b[0;31m           \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_constraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m       )\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/trace_type/default_types.py\u001b[0m in \u001b[0;36mto_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mflattened_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcomp_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0mflattened_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mflattened_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the training loop for GAN\n",
        "def train_gan(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim):\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Ensure batch_size is not greater than dataset size\n",
        "        if batch_size > df_combined.shape[0]:\n",
        "            batch_size = df_combined.shape[0]\n",
        "\n",
        "        # Sample a random half batch of real data\n",
        "        real_batch = df_combined.sample(n=half_batch, replace=True)  # Use replace=True to avoid sampling errors\n",
        "        real_batch = real_batch.astype(np.float32)  # Convert to float32 for compatibility\n",
        "        real_labels = np.ones((half_batch, 1), dtype=np.float32)  # Labels for real data (1)\n",
        "\n",
        "        # Generate fake data\n",
        "        noise = np.random.normal(0, 1, (half_batch, latent_dim))  # Latent space noise for generating fake data\n",
        "        fake_batch = generator.predict(noise)\n",
        "        fake_batch = fake_batch.astype(np.float32)  # Convert to float32 for compatibility\n",
        "        fake_labels = np.zeros((half_batch, 1), dtype=np.float32)  # Labels for fake data (0)\n",
        "\n",
        "        # Train the discriminator on real and fake data\n",
        "        d_loss_real = discriminator.train_on_batch(real_batch, real_labels)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_batch, fake_labels)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))  # Latent space noise for the generator\n",
        "        valid_labels = np.ones((batch_size, 1), dtype=np.float32)  # Labels for the generator to fool the discriminator\n",
        "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "        # Print the progress every epoch\n",
        "        if epoch % 1 == 0:\n",
        "            print(f\"Epoch {epoch} | D Loss: {d_loss[0]} | G Loss: {g_loss}\")\n",
        "\n",
        "# Set parameters for the GAN model\n",
        "latent_dim = 100  # Dimensionality of the random noise\n",
        "output_dim = df_combined.shape[1]  # Number of features in the data\n",
        "\n",
        "# Build and compile the models\n",
        "discriminator = build_discriminator(output_dim)\n",
        "generator = build_generator(latent_dim, output_dim)\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN for 11 epochs\n",
        "epochs = 11\n",
        "batch_size = 32\n",
        "train_gan(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "K2qP0l9-oqV7",
        "outputId": "29adb6e4-1fd2-41e5-c90c-b9dd3ff4db64"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '2023-06-01'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-c24c05950215>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-c24c05950215>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Sample a random half batch of real data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mreal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use replace=True to avoid sampling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mreal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to float32 for compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mreal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Labels for real data (1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_astype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2023-06-01'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Preprocess the data\n",
        "# Drop the 'Date' column (if not needed)\n",
        "df_combined = df_combined.drop(columns=['Date'], axis=1)\n",
        "\n",
        "# If you want to keep 'Date', convert it to numeric\n",
        "# df_combined['Date'] = pd.to_datetime(df_combined['Date'])\n",
        "# reference_date = df_combined['Date'].min()\n",
        "# df_combined['Date'] = (df_combined['Date'] - reference_date).dt.days\n",
        "\n",
        "# Ensure all data is numeric and of type float32\n",
        "df_combined = df_combined.astype(np.float32)\n",
        "\n",
        "# Define the training loop for GAN\n",
        "def train_gan(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim):\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Ensure batch_size is not greater than dataset size\n",
        "        if batch_size > df_combined.shape[0]:\n",
        "            batch_size = df_combined.shape[0]\n",
        "\n",
        "        # Sample a random half batch of real data\n",
        "        real_batch = df_combined.sample(n=half_batch, replace=True)  # Use replace=True to avoid sampling errors\n",
        "        real_labels = np.ones((half_batch, 1), dtype=np.float32)  # Labels for real data (1)\n",
        "\n",
        "        # Generate fake data\n",
        "        noise = np.random.normal(0, 1, (half_batch, latent_dim))  # Latent space noise for generating fake data\n",
        "        fake_batch = generator.predict(noise)\n",
        "        fake_batch = fake_batch.astype(np.float32)  # Convert to float32 for compatibility\n",
        "        fake_labels = np.zeros((half_batch, 1), dtype=np.float32)  # Labels for fake data (0)\n",
        "\n",
        "        # Train the discriminator on real and fake data\n",
        "        d_loss_real = discriminator.train_on_batch(real_batch, real_labels)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_batch, fake_labels)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))  # Latent space noise for the generator\n",
        "        valid_labels = np.ones((batch_size, 1), dtype=np.float32)  # Labels for the generator to fool the discriminator\n",
        "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "        # Print the progress every epoch\n",
        "        if epoch % 1 == 0:\n",
        "            print(f\"Epoch {epoch} | D Loss: {d_loss[0]} | G Loss: {g_loss}\")\n",
        "\n",
        "# Set parameters for the GAN model\n",
        "latent_dim = 100  # Dimensionality of the random noise\n",
        "output_dim = df_combined.shape[1]  # Number of features in the data\n",
        "\n",
        "# Build and compile the models\n",
        "discriminator = build_discriminator(output_dim)\n",
        "generator = build_generator(latent_dim, output_dim)\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN for 11 epochs\n",
        "epochs = 11\n",
        "batch_size = 32\n",
        "train_gan(epochs, batch_size, df_combined, generator, discriminator, gan, latent_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SOOdclco64C",
        "outputId": "1d6a8438-2c14-4287-b7e0-465c053763c9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | D Loss: 3.9848082065582275 | G Loss: 0.7045594453811646\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Epoch 1 | D Loss: 3.350250720977783 | G Loss: 0.6986794471740723\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Epoch 2 | D Loss: 3.112456798553467 | G Loss: 0.6990153789520264\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Epoch 3 | D Loss: 3.0178492069244385 | G Loss: 0.6954140663146973\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Epoch 4 | D Loss: 2.9799389839172363 | G Loss: 0.6956451535224915\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Epoch 5 | D Loss: 2.9362335205078125 | G Loss: 0.6947593688964844\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Epoch 6 | D Loss: 2.9141032695770264 | G Loss: 0.6936522126197815\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Epoch 7 | D Loss: 2.8954367637634277 | G Loss: 0.6916884183883667\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Epoch 8 | D Loss: 2.8590683937072754 | G Loss: 0.6893045902252197\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Epoch 9 | D Loss: 2.8627662658691406 | G Loss: 0.6873112916946411\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Epoch 10 | D Loss: 2.8588504791259766 | G Loss: 0.6856383085250854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new data samples using the trained generator\n",
        "latent_space_samples = np.random.normal(0, 1, (10, latent_dim))  # Generate 10 random noise samples\n",
        "generated_samples = generator.predict(latent_space_samples)\n",
        "\n",
        "print(\"Generated samples:\")\n",
        "print(generated_samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUl_FabYpXQz",
        "outputId": "395fb977-f13b-4be6-bfa8-4d8f2537eb87"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "Generated samples:\n",
            "[[ 0.15560116  0.08922563  0.00285088  0.17718494  0.24709041 -0.04155008\n",
            "   0.23591311]\n",
            " [-0.44223368 -0.6806918  -0.09609998 -0.67217153  0.6783975   0.09297372\n",
            "  -0.10788077]\n",
            " [ 0.1272744   0.2926574   0.41785666 -0.05850643  0.45023146  0.4533676\n",
            "  -0.30686688]\n",
            " [ 0.08855585 -0.30246976  0.09566128  0.21743089  0.29993224  0.27070093\n",
            "  -0.05140532]\n",
            " [-0.25687575 -0.38273355 -0.2941626   0.07576579  0.5934957   0.02866942\n",
            "  -0.38416153]\n",
            " [-0.2443263  -0.10615853 -0.04669548  0.04763838  0.34757546 -0.04140759\n",
            "   0.02360817]\n",
            " [-0.5755819  -0.70635176  0.17860775 -0.6150898   0.59605885 -0.1328187\n",
            "  -0.06354292]\n",
            " [-0.30063245  0.01218119 -0.28941408  0.17875375  0.35048565  0.24537799\n",
            "  -0.43220153]\n",
            " [ 0.03023698 -0.32376248 -0.11038805 -0.03430663  0.29182947  0.17920455\n",
            "  -0.03201198]\n",
            " [-0.37833878 -0.4326068  -0.2658335   0.21874559  0.3943074   0.45660228\n",
            "  -0.7095246 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set the number of samples you want to generate\n",
        "num_samples = 1000\n",
        "\n",
        "# Generate random noise (latent space samples)\n",
        "latent_space_samples = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "\n",
        "# Generate data using the trained generator\n",
        "generated_data = generator.predict(latent_space_samples)\n",
        "\n",
        "# Convert the generated data into a DataFrame\n",
        "generated_df = pd.DataFrame(generated_data, columns=df_combined.columns)\n",
        "\n",
        "# Save the generated data to a CSV file\n",
        "generated_df.to_csv('generated_data.csv', index=False)\n",
        "\n",
        "print(\"Generated data saved to 'generated_data.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7_5yWIQpioH",
        "outputId": "e28fcd35-58fd-46ad-aa08-9a8e60210c06"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Generated data saved to 'generated_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set the number of rows you want to generate\n",
        "num_samples = 1000\n",
        "\n",
        "# Generate random noise (latent space samples)\n",
        "latent_space_samples = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "\n",
        "# Generate data using the trained generator\n",
        "generated_data = generator.predict(latent_space_samples)\n",
        "\n",
        "# Convert the generated data into a DataFrame\n",
        "generated_df = pd.DataFrame(generated_data, columns=df_combined.columns)\n",
        "\n",
        "# Save the generated data to a CSV file in your Colab environment\n",
        "generated_df.to_csv('/content/generated_data.csv', index=False)\n",
        "\n",
        "# Confirm the file has been created\n",
        "print(\"Generated data saved to 'generated_data.csv' at '/content/generated_data.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeuzHAgJpxnV",
        "outputId": "abf469d7-f5d6-4f46-8ec9-346cd810c0e0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Generated data saved to 'generated_data.csv' at '/content/generated_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/generated_data.csv')\n"
      ],
      "metadata": {
        "id": "MWp51RIkp1fI",
        "outputId": "670ed13c-205d-40c2-9306-50d9cf1536e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_32b186d7-3e43-4c0c-b16e-e900cdddbd70\", \"generated_data.csv\", 79786)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}